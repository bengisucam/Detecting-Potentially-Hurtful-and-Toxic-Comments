{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/3   Reading File...]\n",
      "[Step 2/3   Cleaning data...]\n",
      "[Step 3/3   Sampling data...]\n",
      "\n",
      "Sample 1: \n",
      "i read a lot of them but not all i still need to find the fox and the hound the book so i will try to leave and you will tell them that disneyfolly might be on globial block\n",
      "Sample 2: \n",
      " the color of the cloak with which jesus christ was clothed on the day of his execution has caused some persons to argue that a discrepancy exists in the bible record with reference to this garment matthew said that the soldiers “draped him with a scarlet cloak” mt   while mark and john say that it was purple mr  17; joh   however instead of being a discrepancy such a variation in describing the garment’s color merely gives evidence of the individuality of the gospel writers and the fact that they were not in collusion matthew described the cloak as it appeared to him that is according to his evaluation of color and he emphasized the garment’s red hue john and mark subdued the red tint calling it purple “purple” can be applied to any color having components of both blue and red so mark and john agree with matthew that the garment was red to some extent of course background and light reflection could have given it different casts a body of water varies in color at different times depending upon the particular color of the sky and the reflection of light at a given time so when such factors are considered it is seen that the gospel writers were not in conflict in describing the color of the cloak that mocking roman soldiers clothed christ with on the last day of his human life\n",
      "Sample 3: \n",
      " please stop if you continue to vandalise wikipedia you will be blocked talk\n",
      "Sample 4: \n",
      " appreciate it consider itge of mani nouri i read the discussion and i believe that i can make a new page with more sources and credibility\n",
      "Sample 5: \n",
      "just because freaky animal lib ers are over represented on that page does not make them right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     C:\\Users\\bengi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'd' not in vocabulary\"\n",
      "\"word 'aww' not in vocabulary\"\n",
      "\"word 'colour' not in vocabulary\"\n",
      "\"word '' not in vocabulary\"\n",
      "\"word '' not in vocabulary\"\n",
      "\"word 'january' not in vocabulary\"\n",
      "\"word '' not in vocabulary\"\n",
      "\"word '' not in vocabulary\"\n",
      "\"word 'utc' not in vocabulary\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d aww he match the backgrounds colour u noon seemingly sticking having thanks talking   january   utc'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### bu cell main in aynısı ####\n",
    "import pandas\n",
    "import packages.text.textutilities as utilities\n",
    "from packages.text.skeleton import Skeleton\n",
    "import packages.classification.classifier as clfs\n",
    "import packages.augmentation.embedding as embeddings\n",
    "\n",
    "categories=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "df=pandas.read_csv('./Data/train.csv')\n",
    "\n",
    "skeleton=Skeleton()\n",
    "skeleton.build([\n",
    "    utilities.clean_text,\n",
    "    utilities.sample\n",
    "],df_path='./Data/train.csv')\n",
    "\n",
    "\n",
    "word2vec_model=embeddings.EmbeddingAugmentation(load_path=None) #\"./word2vec/word2vec.model\"\n",
    "word2vec_model.replace_sentence_with_top(skeleton.text[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = df.index\n",
    "columns = list(df.columns)\n",
    "values = df.values\n",
    "\n",
    "\n",
    "def createLists(df):\n",
    "    \n",
    "    to_list = []\n",
    "    se_list = []\n",
    "    ob_list = []\n",
    "    th_list = []\n",
    "    ins_list = []\n",
    "    ha_list = []\n",
    "    cl_list = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        comment = df.iloc[i:i+1]\n",
    "        comment_text = str(comment[\"comment_text\"].values[0])\n",
    "        is_toxic =  int(comment[\"toxic\"])\n",
    "        is_severe = int(comment[\"severe_toxic\"])\n",
    "        is_obscene = int(comment[\"obscene\"])\n",
    "        is_threat = int(comment[\"threat\"])\n",
    "        is_insult = int(comment[\"insult\"])\n",
    "        is_hate = int(comment[\"identity_hate\"])\n",
    "        \n",
    "        if(is_toxic):\n",
    "            to_list.append(comment_text)\n",
    "        if(is_severe):\n",
    "            se_list.append(comment_text)\n",
    "        if(is_obscene):\n",
    "            ob_list.append(comment_text)\n",
    "        if(is_threat):\n",
    "            th_list.append(comment_text)\n",
    "        if(is_insult):\n",
    "            ins_list.append(comment_text)\n",
    "        if(is_hate):\n",
    "            ha_list.append(comment_text)\n",
    "        else:\n",
    "            cl_list.append(comment_text)\n",
    "        \n",
    "    \n",
    "#     toxic_df = pandas.DataFrame(to_list)\n",
    "#     severe_df = pandas.DataFrame(se_list)\n",
    "#     obscene_df = pandas.DataFrame(ob_list)\n",
    "#     threat_df = pandas.DataFrame(th_list)\n",
    "#     insult_df = pandas.DataFrame(ins_list)\n",
    "#     hate_df = pandas.DataFrame(ha_list)\n",
    "    \n",
    "    return to_list, se_list, ob_list ,th_list, ins_list, ha_list, cl_list\n",
    "    \n",
    "\n",
    "toxic_ls,severe_ls,obscene_ls,threat_ls,insult_ls,hate_ls, cl_ls = createLists(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15294\n",
      "1595\n",
      "8449\n",
      "478\n",
      "7877\n",
      "1405\n",
      "44\n",
      "COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(len(toxic_ls))\n",
    "print(len(severe_ls))\n",
    "print(len(obscene_ls))\n",
    "print(len(threat_ls))\n",
    "print(len(insult_ls))\n",
    "print(len(hate_ls))\n",
    "print(len(cl_ls))\n",
    "\n",
    "\n",
    "print(len(toxic_ls[0]))\n",
    "print(toxic_ls[0])\n",
    "\n",
    "\n",
    "\n",
    "def createAndAugmentDf(ls):\n",
    "    \n",
    "    fix_size = 5000    \n",
    "    size = len(ls)    \n",
    "    diff = fix_size - size\n",
    "    \n",
    "    new_ls = []\n",
    "    ind = 0\n",
    "    while(diff):\n",
    "        comment = ls[ind]\n",
    "        if(len(comment) > 8):\n",
    "            # add original comment\n",
    "            new_ls.append(comment)\n",
    "            # add the augmented one as well\n",
    "            \n",
    "            \n",
    "            diff -= 2\n",
    "            ind += 1\n",
    "    \n",
    "    \n",
    "    df = pandas.DataFrame(new_ls)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "toxic_df = createAndAugmentDf(toxic_ls)\n",
    "    \n",
    "print(toxic_df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey... what is it..\r\n",
      "@ | talk .\r\n",
      "What is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\r\n",
      "\r\n",
      "Ask Sityush to clean up his behavior than issue me nonsensical warnings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     C:\\Users\\bengi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'Hey...' not in vocabulary\"\n",
      "\"word 'it..\\r\\n@' not in vocabulary\"\n",
      "\"word '|' not in vocabulary\"\n",
      "\"word '.\\r\\nWhat' not in vocabulary\"\n",
      "\"word 'it...' not in vocabulary\"\n",
      "\"word 'of' not in vocabulary\"\n",
      "\"word 'WP' not in vocabulary\"\n",
      "\"word 'TALIBANS...who' not in vocabulary\"\n",
      "\"word 'destroying,' not in vocabulary\"\n",
      "\"word 'self-appointed' not in vocabulary\"\n",
      "\"word 'purist' not in vocabulary\"\n",
      "\"word 'GANG' not in vocabulary\"\n",
      "\"word 'UP' not in vocabulary\"\n",
      "\"word 'abt' not in vocabulary\"\n",
      "\"word 'ANTI-SOCIAL' not in vocabulary\"\n",
      "\"word 'and' not in vocabulary\"\n",
      "\"word 'DESTRUCTIVE' not in vocabulary\"\n",
      "\"word '(non)-contribution' not in vocabulary\"\n",
      "\"word 'WP?\\r\\n\\r\\nAsk' not in vocabulary\"\n",
      "\"word 'Sityush' not in vocabulary\"\n",
      "\"word 'to' not in vocabulary\"\n",
      "\"word 'warnings...' not in vocabulary\"\n",
      "<packages.augmentation.embedding.EmbeddingAugmentation object at 0x0000025398960400>\n"
     ]
    }
   ],
   "source": [
    "print(toxic_ls[1])\n",
    "word2vec_model=embeddings.EmbeddingAugmentation(load_path=None) #\"./word2vec/word2vec.model\"\n",
    "word2vec_model.replace_sentence_with_top(toxic_ls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n",
      "159571\n",
      "0         explanation\\r why the edits made under my user...\n",
      "1         d aww he matches this background colour i am s...\n",
      "2         hey man i am really not trying to edit war it ...\n",
      "3         more\\r i ca not make any real suggestions on i...\n",
      "4         you sir are my hero any chance you remember wh...\n",
      "                                ...                        \n",
      "159566    and for the second time of asking when your vi...\n",
      "159567    you should be ashamed of yourself \\r \\r that i...\n",
      "159568    spitzer \\r \\r umm theres no actual article for...\n",
      "159569    and it looks like it was actually you who put ...\n",
      "159570    and i really do not think you understand i cam...\n",
      "Name: comment_text, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(len(skeleton.text))\n",
    "\n",
    "print(len(df.comment_text))\n",
    "\n",
    "print(skeleton.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
