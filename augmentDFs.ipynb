{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1/2   Cleaning data...]\n",
      "[Step 2/2   Sampling data...]\n",
      "Sample 1: \n",
      "kristy wright was voted in the top  sexy soap stars\n",
      "Sample 2: \n",
      " hello again i posted a question to you last july http enwikipediaorg wiki user talk rhaworth archive to  july samuel purdey but i have had no response from you whatsoever i think you missed it due to a vacation or something could you please advise all the best\n",
      "Skeleton build is done!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e4d2abf3dc82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mGet\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mn_category\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \"\"\"\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0msamples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mskeleton\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_by_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_category\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'toxic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\IdeaProjects\\Detecting-Potentially-Hurtful-and-Toxic-Comments\\packages\\text\\skeleton.py\u001b[0m in \u001b[0;36msplit_by_keys\u001b[1;34m(self, n_category)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mdf_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0meach_key\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mdf_splits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meach_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meach_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_category\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf_splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[0;32m   4968\u001b[0m             )\n\u001b[0;32m   4969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4970\u001b[1;33m         \u001b[0mlocs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4971\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_copy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "#### bu cell main in aynısı ####\n",
    "import pandas as pd\n",
    "from packages.text.textutilities import *\n",
    "from packages.text.skeleton import Skeleton\n",
    "import packages.classification.classifier as clfs\n",
    "import packages.augmentation.embedding as embeddings\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "categories=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "random_state=20\n",
    "\n",
    "\"\"\"\n",
    "Build a skeleton framework to ease up workflow\n",
    "\"\"\"\n",
    "skeleton=Skeleton(categories,random_state)\n",
    "skeleton.build([\n",
    "    TextCleaner(),\n",
    "    Sampler()\n",
    "    # Trimmer(threshold=12)\n",
    "],df_path='./Data/train.csv')\n",
    "\n",
    "\"\"\"\n",
    "Get <n_category> random samples from each category\n",
    "\"\"\"\n",
    "samples=skeleton.split_by_keys(n_category=3000)\n",
    "samples['toxic']\n",
    "\n",
    "word2vec_model=embeddings.EmbeddingAugmentation(load_path=\"./word2vec/word2vec.model\")\n",
    "word2vec_model.populate(keys=categories,\n",
    "                        data=samples,\n",
    "                        target=5000,\n",
    "                        random=random_state,\n",
    "                        augmentation_per_sentence=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = skeleton.df.index\n",
    "columns = list(skeleton.df.columns)\n",
    "values = skeleton.df.values\n",
    "\n",
    "\n",
    "def createLists(df):\n",
    "    \n",
    "    to_list = []\n",
    "    se_list = []\n",
    "    ob_list = []\n",
    "    th_list = []\n",
    "    ins_list = []\n",
    "    ha_list = []\n",
    "    cl_list = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        comment = df.iloc[i:i+1]\n",
    "        comment_text = str(comment[\"comment_text\"].values[0])\n",
    "        is_toxic =  int(comment[\"toxic\"])\n",
    "        is_severe = int(comment[\"severe_toxic\"])\n",
    "        is_obscene = int(comment[\"obscene\"])\n",
    "        is_threat = int(comment[\"threat\"])\n",
    "        is_insult = int(comment[\"insult\"])\n",
    "        is_hate = int(comment[\"identity_hate\"])\n",
    "        \n",
    "        if(is_toxic):\n",
    "            to_list.append(comment_text)\n",
    "        if(is_severe):\n",
    "            se_list.append(comment_text)\n",
    "        if(is_obscene):\n",
    "            ob_list.append(comment_text)\n",
    "        if(is_threat):\n",
    "            th_list.append(comment_text)\n",
    "        if(is_insult):\n",
    "            ins_list.append(comment_text)\n",
    "        if(is_hate):\n",
    "            ha_list.append(comment_text)\n",
    "        else:\n",
    "            cl_list.append(comment_text)\n",
    "        \n",
    "    \n",
    "    return to_list, se_list, ob_list ,th_list, ins_list, ha_list, cl_list\n",
    "    \n",
    "\n",
    "toxic_ls,severe_ls,obscene_ls,threat_ls,insult_ls,hate_ls, cl_ls = createLists(skeleton.df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15294\n",
      "1595\n",
      "8449\n",
      "478\n",
      "7877\n",
      "1405\n",
      "158166\n",
      "44\n",
      "cocksucker before you piss around on my work\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-69ef6bd0b9c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mtoxic_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateAndAugmentDf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoxic_ls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoxic_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-69ef6bd0b9c4>\u001b[0m in \u001b[0;36mcreateAndAugmentDf\u001b[1;34m(ls)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mnew_ls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mcomment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(len(toxic_ls))\n",
    "print(len(severe_ls))\n",
    "print(len(obscene_ls))\n",
    "print(len(threat_ls))\n",
    "print(len(insult_ls))\n",
    "print(len(hate_ls))\n",
    "print(len(cl_ls))\n",
    "\n",
    "\n",
    "print(len(toxic_ls[0]))\n",
    "print(toxic_ls[0])\n",
    "\n",
    "\n",
    "\n",
    "def createAndAugmentDf(ls, model, random):\n",
    "    \n",
    "    fix_size = 5000    \n",
    "    size = len(ls)    \n",
    "    diff = fix_size - size\n",
    "    \n",
    "    new_ls = []\n",
    "    ind = 0\n",
    "    while(diff):\n",
    "        comment = ls[ind]\n",
    "        if(len(comment) > 8):\n",
    "            # add original comment\n",
    "            new_ls.append(comment)\n",
    "            # add the augmented one as well\n",
    "            words = comment.split(' ')\n",
    "            word_list = []\n",
    "            \n",
    "            for i in range(3):\n",
    "                word = words.sample(n=1, random_state=random)\n",
    "                similar = model.wv.most_similar(positive=[word], topn=3)\n",
    "                for w in similar:\n",
    "                    word_list.append(str(w).lower())\n",
    "                \n",
    "\n",
    "                \n",
    "            \n",
    "            \n",
    "            new_ls.append(augmented)\n",
    "            \n",
    "            diff -= 2\n",
    "            ind += 1\n",
    "    \n",
    "    \n",
    "    df = pandas.DataFrame(new_ls)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "toxic_df = createAndAugmentDf(toxic_ls, model, random_state)\n",
    "    \n",
    "print(toxic_df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey... what is it..\r\n",
      "@ | talk .\r\n",
      "What is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\r\n",
      "\r\n",
      "Ask Sityush to clean up his behavior than issue me nonsensical warnings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     C:\\Users\\bengi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'Hey...' not in vocabulary\"\n",
      "\"word 'it..\\r\\n@' not in vocabulary\"\n",
      "\"word '|' not in vocabulary\"\n",
      "\"word '.\\r\\nWhat' not in vocabulary\"\n",
      "\"word 'it...' not in vocabulary\"\n",
      "\"word 'of' not in vocabulary\"\n",
      "\"word 'WP' not in vocabulary\"\n",
      "\"word 'TALIBANS...who' not in vocabulary\"\n",
      "\"word 'destroying,' not in vocabulary\"\n",
      "\"word 'self-appointed' not in vocabulary\"\n",
      "\"word 'purist' not in vocabulary\"\n",
      "\"word 'GANG' not in vocabulary\"\n",
      "\"word 'UP' not in vocabulary\"\n",
      "\"word 'abt' not in vocabulary\"\n",
      "\"word 'ANTI-SOCIAL' not in vocabulary\"\n",
      "\"word 'and' not in vocabulary\"\n",
      "\"word 'DESTRUCTIVE' not in vocabulary\"\n",
      "\"word '(non)-contribution' not in vocabulary\"\n",
      "\"word 'WP?\\r\\n\\r\\nAsk' not in vocabulary\"\n",
      "\"word 'Sityush' not in vocabulary\"\n",
      "\"word 'to' not in vocabulary\"\n",
      "\"word 'warnings...' not in vocabulary\"\n",
      "<packages.augmentation.embedding.EmbeddingAugmentation object at 0x0000025398960400>\n"
     ]
    }
   ],
   "source": [
    "print(toxic_ls[1])\n",
    "word2vec_model=embeddings.EmbeddingAugmentation(load_path=None) #\"./word2vec/word2vec.model\"\n",
    "word2vec_model.replace_sentence_with_top(toxic_ls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n",
      "159571\n",
      "0         explanation\\r why the edits made under my user...\n",
      "1         d aww he matches this background colour i am s...\n",
      "2         hey man i am really not trying to edit war it ...\n",
      "3         more\\r i ca not make any real suggestions on i...\n",
      "4         you sir are my hero any chance you remember wh...\n",
      "                                ...                        \n",
      "159566    and for the second time of asking when your vi...\n",
      "159567    you should be ashamed of yourself \\r \\r that i...\n",
      "159568    spitzer \\r \\r umm theres no actual article for...\n",
      "159569    and it looks like it was actually you who put ...\n",
      "159570    and i really do not think you understand i cam...\n",
      "Name: comment_text, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(len(skeleton.text))\n",
    "\n",
    "print(len(df.comment_text))\n",
    "\n",
    "print(skeleton.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
