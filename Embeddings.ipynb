{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python37\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\python37\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: gensim in c:\\python37\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\python37\\lib\\site-packages (from gensim) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\python37\\lib\\site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\python37\\lib\\site-packages (from gensim) (1.9.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\python37\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\python37\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in c:\\python37\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied: boto3 in c:\\python37\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.10.12)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\python37\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\python37\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\python37\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python37\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in c:\\python37\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\python37\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.12 in c:\\python37\\lib\\site-packages (from boto3->smart-open>=1.8.1->gensim) (1.13.12)\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in c:\\python37\\lib\\site-packages (from botocore<1.14.0,>=1.13.12->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\python37\\lib\\site-packages (from botocore<1.14.0,>=1.13.12->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n",
      "[nltk_data]     C:\\Users\\bengi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43981\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.data import find\n",
    "\n",
    "nltk.download('word2vec_sample')\n",
    "\n",
    "\n",
    "word2vec_small = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "pre_trained_model_small = gensim.models.KeyedVectors.load_word2vec_format(word2vec_small, binary=False)\n",
    "\n",
    "\n",
    "print(len(pre_trained_model_small.vocab))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(model['dick'])\n",
    "len(pre_trained_model_small['shit'])\n",
    "len(pre_trained_model_small['bitch'])\n",
    "len(pre_trained_model_small['nigger'])\n",
    "# len(pre_trained_model_small['asshole'])\n",
    "len(pre_trained_model_small['cock'])\n",
    "len(pre_trained_model_small['bastard'])\n",
    "# len(pre_trained_model_small['slut'])\n",
    "len(pre_trained_model_small['whore'])\n",
    "len(pre_trained_model_small['butt'])\n",
    "# len(pre_trained_model_small['dickhead'])\n",
    "# len(pre_trained_model_small['faggot'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ass', 0.7703798413276672), ('butts', 0.714277446269989), ('backside', 0.6110208034515381)]\n",
      "[('reading', 0.6985589861869812), ('reread', 0.6906845569610596), ('reads', 0.6628413200378418)]\n",
      "[('John', 0.6729259490966797), ('Charles', 0.6258001327514648), ('Paul', 0.6121022701263428)]\n",
      "[('crap', 0.8299816250801086), ('fuck', 0.7604621648788452), (\"y'know\", 0.7228367328643799)]\n",
      "[('great', 0.7291511297225952), ('bad', 0.7190051674842834), ('terrific', 0.6889115571975708)]\n",
      "0.084653266\n",
      "0.4155177\n"
     ]
    }
   ],
   "source": [
    "print(pre_trained_model_small.most_similar(positive=['butt'], topn = 3))\n",
    "\n",
    "print(pre_trained_model_small.most_similar(positive=['read'], topn = 3))\n",
    "\n",
    "print(pre_trained_model_small.most_similar(positive=['George'], topn = 3))\n",
    "\n",
    "print(pre_trained_model_small.most_similar(positive=['shit'], topn = 3))\n",
    "\n",
    "print(pre_trained_model_small.most_similar(positive=['good'], topn = 3))\n",
    "\n",
    "print(pre_trained_model_small.similarity(\"shit\", 'flower'))\n",
    "print(pre_trained_model_small.similarity(\"education\", 'university'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         Explanation\\r\\nWhy the edits made under my use...\n",
      "1         D'aww! He matches this background colour I'm s...\n",
      "2         Hey man, I'm really not trying to edit war. It...\n",
      "3         \"\\r\\nMore\\r\\nI can't make any real suggestions...\n",
      "4         You, sir, are my hero. Any chance you remember...\n",
      "                                ...                        \n",
      "159566    \":::::And for the second time of asking, when ...\n",
      "159567    You should be ashamed of yourself \\r\\n\\r\\nThat...\n",
      "159568    Spitzer \\r\\n\\r\\nUmm, theres no actual article ...\n",
      "159569    And it looks like it was actually you who put ...\n",
      "159570    \"\\r\\nAnd ... I really don't think you understa...\n",
      "Name: comment_text, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#### train word2vec on our dataset ####\n",
    "import pandas\n",
    "categories=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "df=pandas.read_csv('./Data/train.csv')\n",
    "print(df.comment_text)\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "        df.comment_text,\n",
    "        size=150,\n",
    "        window=10,\n",
    "        min_count=1,\n",
    "        workers=10,\n",
    "        iter=10\n",
    ")\n",
    " \n",
    "model.save('toxic.embedding')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336\n"
     ]
    }
   ],
   "source": [
    "new_model = gensim.models.Word2Vec.load('toxic.embedding')\n",
    "print(len(new_model.wv.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbq \r",
      " \r",
      " be a man and lets discuss it maybe over the phone\n",
      "bbq แบน แบน be e man and lets discuss it maybe over the phone\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('./Data/train.csv')\n",
    "#take non toxic data\n",
    "txt=(df.loc[df[\"toxic\"]==0]).iloc[10].comment_text\n",
    "import packages.text.textutilities as tu\n",
    "txt=tu.clean_sample(txt)\n",
    "words=txt.split(' ')\n",
    "\n",
    "for i,word in enumerate(words):\n",
    "    if word is None:\n",
    "        continue\n",
    "    try:\n",
    "        #similar_words=pre_trained_model_small.most_similar(positive=[word], topn = 5)\n",
    "        similar_words=new_model.wv.most_similar(positive=[word], topn = 5)\n",
    "        top_,_=similar_words[0]\n",
    "        words[i]=top_\n",
    "    except KeyError as ke:\n",
    "        pass\n",
    "print(txt)\n",
    "str1 = ''.join(str(e)+\" \" for e in words)          \n",
    "print(tu.clean_sample(str1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
