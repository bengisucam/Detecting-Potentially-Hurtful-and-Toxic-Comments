{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import packages.text.textutilities as utilities\n",
    "from packages.text.skeleton import Skeleton\n",
    "import packages.classification.classifier as clfs\n",
    "\n",
    "categories=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "df=pandas.read_csv('./Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
<<<<<<< Updated upstream
      "[Step 1/3   Reading File...]\n",
      "[Step 2/3   Cleaning data...]\n",
      "[Step 3/3   Sampling data...]\n\nSample 1: \nwow i honestly do not know where to begin to respond to that ~ so lets start with your talking points first off i am not trying to americanize the article somethings were changed like donburi to rice bowl but they are largely a manual of style issue than anything else next off i am not egotistical so showing off my editing skills is a non issue plus this is wikipedia someone else editing your work is a fact of life third deja vu artworks is based in japan if i worked for them that would seem that i lived in japan as well in which case why would i want to americanize the article which brings us to trying to improve the article as noted before a good chuck of infomraiton in the original revisions was irrelevant you do not need a song listing from a flash animation in an article that is a general overview of a subject also the deja vu information was removed because it was unsourced and i ca not seem to find any indication of resolution for the topic it happened in  and it is now  either it was resolved or it fell by the way side where as the netrunner controversy is still on going and received a great deal of notice when it happened if there is a bunch of information on deja vu that i am missing then by all means point out some links and we can work it into the article now in regards to most of the changes this article toes very close to the fancruft category and as such most of the edits are there to try and pull it away from that category in case you have not noticed most of the information in the list of os tans article is gone because that article was deleted; one of the reasons for which was fancruft now all that said there is no reason why the descriptions of the top five tans can be expanded a bit but you must remember that should not be a full bio of the characters plus the fact that they can be changed at an artists whim means that you ca not write in the matter of osx tan is always seen poking xp tan with a stick that is just one thing that an artist can do\nSample 2: \nvandalism please refrain from adding nonsense to wikipedia as you did to funny farm film it is considered vandalism if you would like to experiment use the sandbox\nSample 3: \njohnboywalton55 says you re a spastic also father daley reportedly had a big lump on the side of his head in the shape of connaught\nSample 4: \ni suggest bejnar to first prove his hypothesis about demographic status of cities in west azerbaijan province and then force his statistics which i believe it is fake by the way we have  cities with population higher than or above  in the province numbers and population is mentioned in your favorite table is incorrect and incompatible with your mentioned references see following link which is reference number  in this article west azerbaijan statistics\nSample 5: \nadditionally i have added four reliable sources including fox news and ny mag talking about the issue\n"
=======
      "[Step 1/4   Reading File...]\n",
      "[Step 2/4   Cleaning data...]\n",
      "[Step 3/4   Trimming data...]\n\n"
>>>>>>> Stashed changes
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "skeleton=Skeleton()\n",
    "skeleton.build([\n",
    "    utilities.clean_text,\n",
    "    utilities.sample\n",
    "],df_path='./Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
<<<<<<< Updated upstream
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to\n[nltk_data]     /home/fcaglayan/nltk_data...\n[nltk_data]   Package word2vec_sample is already up-to-date!\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import packages.augmentation.embedding as embeddings\n",
    "thing=embeddings.EmbeddingAugmentation()\n",
    "words=thing.get_similar_words(\"fuck\",n=3)\n",
    "if words is not None:\n",
    "    print(words)\n",
    "thing.train_new(skeleton.text)\n",
    "\n",
    "words=thing.get_similar_words_trained(\"fuck\",n=3)\n",
    "if words is not None:\n",
    "    print(words)\n",
    "thing.save_trained_model()"
   ]
=======
   "outputs": [],
   "source": []
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(list(thing.model.wv.vocab))\n",
    "\n",
    "print(list(thing.trained_model.wv.vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
<<<<<<< Updated upstream
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
=======
>>>>>>> Stashed changes
    }
   },
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "BOOSTING_CLASSIFIER_COUNT=100\n",
=======
    "skeleton.classify([\n",
    "    clfs.DecisionTree(),\n",
    "    clfs.BoostingDecisionTree(),\n",
    "    clfs.NaiveBayes(),\n",
    "    clfs.AveragingEstimator(),\n",
    "    clfs.SVM(),\n",
    "    \n",
    "],keys=categories)\n",
>>>>>>> Stashed changes
    "\n",
    "# \n",
    "# skeleton.classify([\n",
    "#     clfs.AdaBoostSVM(n_estimators=2),\n",
    "#     clfs.AdaBoostNaiveBayes(n_estimators=10),\n",
    "#     clfs.AdaBoostNaiveBayes(n_estimators=20),\n",
    "#     clfs.AdaBoostDecisionTree(n_estimators=20),\n",
    "#     clfs.DecisionTree(),\n",
    "#     clfs.NaiveBayes(),\n",
    "#     clfs.AveragingEstimator(),\n",
    "#     clfs.SVM(),\n",
    "#     \n",
    "# ],keys=categories)\n",
    "# \n",
    "# skeleton.save_progress(\"progress.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "utilities.clean_sample(\"}}{{Connected contributor|WWB Too|WWB Too|editedhere=no\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}